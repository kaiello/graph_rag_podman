1. Create input and output bucket for S3
- # Uploads the local file to s3://my-docling-input-data-2025/incoming/my_test_document.pdf
aws s3 cp test_data/sample.pdf s3://my-docling-input-data-2025/incoming/my_test_document.pdf



deployment Order & Workflow
Deploy App 2 First:

This creates the GraphInputBucket and sets up the listener.

Deploy App 1 Second:

Set App 1's OutputBucketName parameter to the value of App 2's GraphInputBucket.

Deployment Note: When you deploy this stack, you will paste the bucket name created by App 2 into the OutputBucketName parameter.

Result:

App 1 runs -> Drops JSONL in GraphInputBucket -> Automatically triggers App 2 -> App 2 splits files to FinalOutputBucketName


2. If error, Remove-Item -Recurse -Force .aws-sam
3. 757905896558.dkr.ecr.us-east-1.amazonaws.com/docling-ingest - repository URI

Additional runs
# Delete the build cache
Remove-Item -Recurse -Force .aws-sam


# verify podman start
podman machine start
podman machine ssh "ping -c 3 amazon.com" # Quick connectivity test


# Make sure you are in the app2-graph-builder directory
sam build --use-container --no-cached

sam local invoke GraphBuilderFunction \
  --event events/s3_event.json \
  --profile default

*(Note: If you are not using an AWS profile named `default`, remove `--profile default` or change it to your active profile name.)*

### Summary
* **Local Testing:** You use `sam local invoke` + `event.json`. The AWS Console trigger configuration is irrelevant here.
* **Cloud Deployment:** Only when you `sam deploy` to the cloud do you need to worry about the Console Trigger configuration.


If app.py is not updating
& "C:\Program Files\RedHat\Podman\podman.exe" rm --all --force

& "C:\Program Files\RedHat\Podman\podman.exe" system prune --all --volumes --force
